# ETL Pipeline for DAP Expansion
Проект представляет собой автоматизированный ETL-пайплайн для синхронизации данных между Google Sheets и PostgreSQL с последующей визуализацией в Grafana.

# Основные функции
- Ежечасная проверка новых данных в Google Sheets.
- Загрузка новых данных в PostgreSQL.
- Визуализация данных в Grafana.

# Цель проекта
В рамках расширения бизнес-проекта **DAP** возникла необходимость переноса данных из Google Sheets в реляционную базу данных PostgreSQL для более эффективного управления и анализа информации. 
Текущая система хранения данных в Google Sheets достигла своих пределов, и требуется автоматизированный процесс для синхронизации данных между различными системами.

Для решения этих проблем был разработан ETL-пайплайн, который обеспечивает:
1. Автоматическую синхронизацию данных между Google Sheets и PostgreSQL.
2. Возможность детального анализа данных с помощью инструментов визуализации (Grafana).
3. Улучшение производительности и надёжности хранения данных.

# Архитектура решения
Проект состоит из следующих компонентов:
1. **Google Sheets**: Исходный источник данных, где собираются заказы от клиентов.
2. **PostgreSQL**: Реляционная база данных для хранения и управления данными.
3. **ETL-пайплайн**: Автоматический процесс для извлечения, преобразования и загрузки данных.
4. **Grafana**: Инструмент для визуализации данных и создания дашбордов.

# Технические сведение 
Скрипты разработаны для пользователей операционной системы Windows в соответствии с требованиями заказчика. Проект в настоящее время находится в эксплуатации у заказчика. На текущий момент проект предполагает использование базы данных PostgreSQL, интегрированной на локальном уровне с программным обеспечением пользователя. Связь между программным обеспечением и базой данных осуществляется на локальной машине пользователя.

# Источники данных
Данные для проекта предоставлены компанией **DAP** (ссылка: [https://dapnsk.ru/]). Компания предоставила доступ к агрегированным данным, что позволяет использовать их для аналитических целей. 
Все данные были обезличены и подготовлены для использования в учебных и демонстрационных целях. Компания **DAP** разрешила публиковать результаты анализа с указанием источника данных.

# Требования
Для запуска проекта необходимы следующие компоненты:
- Python 3.8 или выше
- PostgreSQL 13+
- Grafana 9+
- Google Cloud Account с доступом к API Sheets

# Зависимости
Все зависимости перечислены в файле `requirements.txt`:
- psycopg2-binary: Для работы с PostgreSQL.
- gspread: Для работы с Google Sheets.
- oauth2client: Для аутентификации в Google API.

# Структура проекта 
etl/
├── srm
│   ├── etl.py               # Основной ETL-процесс
│   ├── google_sheets.py     # Логика работы с Google Sheets
│   ├── postgres.py          # Логика работы с PostgreSQL
│   ├── utils.py             # Вспомогательные функции
│   └── config.py            # Конфигурация  
├── tests                    # Тесты  
   ├── test_unit                              тока unit-тесты(((())))  
├── grafana                    
├──  scripts                 # Скрипты для запуска и настройки
   ├── install_dependencies.ps1  # Установка зависимостей
   ├── setup_db.ps1              # Создание базы данных
   └── run_etl.ps1               # Запуск ETL-процесса               
├── requirements.txt         # Зависимости Python
└── README.md                # Документация проекта

# Планы дальнейшего развития проекта
1. В ближайшей перспективе требуется перенос базы данных с локальной машины пользователя на сервер для обеспечения глобального доступа и повышения отказоустойчивости системы.
2. Необходимо реализовать функционал автоматического считывания данных и определения их типов для обеспечения корректной работы системы вне зависимости от количества столбцов и формата данных, импортируемых из Google Sheets. Данный функционал должен быть разработан с учетом возможности массового использования системы множеством пользователей, обеспечивая универсальность и удобство эксплуатации.

# Алгоритм установки и настройки ETL Pipeline 
1. Установите Python 3.8 или выше: [официальный сайта](https://www.python.org/downloads/).
2. Установите PostgreSQL [официальный сайта](https://www.postgresql.org/download/).
3. Установите зависимости Python:  
   Выполните команду в PowerShell:  
   ```powershell
   .\scripts\install_dependencies.ps1
4. Создайте базу данных и пользователя:  
   Выполните команду в PowerShell:  
   ```powershell
   .\scripts\setup_db.ps1
   ``` 
5. Проверьте подключение к базе данных:  
   Убедитесь, что база данных доступна по указанным в `config.py` параметрам.

6. Создайте проект в Google Cloud Console:  
   Перейдите в [Google Cloud Console](https://console.cloud.google.com/).
   - В разделе **IAM и администрирование** → **Сервисные аккаунты** → **Создать аккаунт**.  
   - Назначьте роль **Editor**.  
   - Создайте ключ доступа в формате JSON и скачайте файл.

7. Настройте доступ к Google Sheets:  
   - Откройте Google Sheets, с которыми будете работать.  В данном случае таблица DAP.
   - Нажмите **Настройки доступа** → Добавьте сервисный аккаунт (используйте email из JSON-файла).  

8. Сохраните JSON-файл с ключом в папку `etl/srm/` и укажите путь к нему в `config.py`.

9.  Скачайте и установите Grafana с [Grafana](https://grafana.com/grafana/download).
   - Откройте Grafana в браузере (по умолчанию `http://localhost:3000`).  
   - Войдите в систему (логин/пароль по умолчанию: `admin/admin`).  
   - Настройте личный хост, если требуется (например, измените порт или домен).

10. Запустите скрипт для импорта дашборда.
    API-токен Grafana реализованного даш-борда сохранен в `token.txt`. При необходимости в файл `import_dashboard.ps1` можно добавить API-токен. другого даш-борда. 
     ```powershell
     .\import_dashboard.ps1
     ```  
   - После запуска скрипта дашборд автоматически загрузится в Grafana.  
   - Чтобы его найти:  
     - Перейдите в раздел **Dashboards** → **Manage**.  
     - Найдите ваш дашборд по названию (оно указано в JSON-файле).  
   - Дашборд будет готов к использованию сразу после импорта. 
   - Убедитесь, что Grafana запущена и доступна по указанному хосту.  
   - Если дашборд не отображается, проверьте:  
      - Корректность API-токена.  
      - Наличие подключенного источника данных (например, PostgreSQL).
11. Запустите ETL-скрипт:  
   Выполните команду в PowerShell:  
   ```powershell
   .\scripts\run_etl.ps1
   ```  
   Скрипт будет синхронизировать данные из Google Sheets в PostgreSQL. Проверьте корректность выводимых данных.
12. Для автоматического запуска ETL-процесса:  
   Настройте планировщик задач ( Task Scheduler на Windows).
   Можно также запукать самостоятельно, данные будут интегрироваться при каждом подключении. 
13. Для тестирования запустите unit-тесты:  
   ```powershell
   py -m pytest tests/test_unit/
   ```
